{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNwwatoSfSc/R8WHVIwlaoD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SydanJainen/4EU-DeepLearningCourse/blob/main/Assignments/AssignementCNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMteOzeJ8hrL"
      },
      "source": [
        "# Assignments\n",
        "\n",
        "### We are given a dataset containing 510 images which are categorized according two main characteristics/properties: “C1” and “C2”. The following inputs are available:\n",
        "* #### `input image', a 110 × 110 × 3 real-valued tensor. The last dimension denotes the number of input channels; the images belong to different combinations of the two properties C1 and C2. For C1 and C2 we have 5 and 6 possible values, respectively;  among the 30 possible couples (C1,C2), we have images for only 24 of them. We have around 20 samples (little more or less) for each of the available couples;  values in each tensor entry are integers in [0, 255];\n",
        "* #### C1, string, the value for the C1 property. 5 possible values;\n",
        "* #### C2, string, the value for the C2 property. 6 possible values.\n",
        "\n",
        "### Design a deep neural network model to predict the class of an image, given by the couple (C1,C2).\n",
        "\n",
        "### Provide a sketch of each of the following points, then implement your solution.\n",
        "1. MODEL: Which architecture do you consider the most appropriate for this task, and why;\n",
        "2. INPUT:\n",
        "* After a potential preprocessing step, which is the input of the model, and\n",
        "how is it represented;\n",
        "3. OUTPUT: How would you design the output layer and why;\n",
        "4. LOSS: Which loss function would you use to train your model and why;\n",
        "5. MODEL CONFIGURATION:\n",
        " Model composition (composition of layers, regardless their number,\n",
        "or their dimension, which can be object of tuning)\n",
        "6. MODEL EVALUATION: How would you assess (in which setting) the\n",
        "generalization capabilities of the model on unseen data?\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "## go to your datapath\n",
        "%cd /content/drive/MyDrive/INSEGNAMENTI/4EU+/\n",
        "\n",
        "# Here you should see the desired files\n",
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OI9U1PiwZ8EJ",
        "outputId": "6580cecc-3ea9-41dd-fb90-a722a0b42d67"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/INSEGNAMENTI/4EU+\n",
            "input.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ApgM0K_w8hrQ",
        "outputId": "e5ba537e-2d11-43c6-d442-be2d228dafc3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length image:510\n",
            "length C1:510\n",
            "length C2:510\n"
          ]
        }
      ],
      "source": [
        "import pickle as pk\n",
        "# Open the pickle data\n",
        "with open('input.pkl', 'rb') as f:\n",
        "  data = pk.load(f)\n",
        "\n",
        "images = data['imgs']\n",
        "C1 = data['C1']\n",
        "C2 = data['C2']\n",
        "print(f\"length image:{len(images)}\")\n",
        "print(f\"length C1:{len(C1)}\")\n",
        "print(f\"length C2:{len(C2)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import torchvision  #to get the MNIST dataset\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.transforms import ToTensor, Normalize, Compose\n",
        "from torch.utils.data import random_split"
      ],
      "metadata": {
        "id": "E5t2r4xPY7xy"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for a in np.unique(C1):\n",
        "  print(a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_9ZK3RfIRE9",
        "outputId": "68e653fe-76fe-4745-d70f-31b32b43b9b1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dress\n",
            "pants\n",
            "shirt\n",
            "shoes\n",
            "shorts\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for a in np.unique(C2):\n",
        "  print(a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3o2q8J0jKzR5",
        "outputId": "4c0fe1d4-5137-4010-bc18-3042b2d71335"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "black\n",
            "blue\n",
            "brown\n",
            "green\n",
            "red\n",
            "white\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create a dictionary to combine C1 and C2 labels into a single string\n",
        "label_combinations = {}\n",
        "for i, (c1, c2) in enumerate(zip(C1, C2)):\n",
        "    label_combinations[i] = f\"{c1} {c2}\"  # Combine C1 and C2 with a space\n",
        "\n",
        "# Create a unique set of all possible label combinations\n",
        "unique_labels = set(label_combinations.values())\n",
        "\n",
        "# Create a mapping from label string to a unique label index\n",
        "label_to_index = {label: i for i, label in enumerate(unique_labels)}\n",
        "\n",
        "# Convert images and labels to appropriate data structures\n",
        "# (e.g., convert images to tensors and labels to one-hot encoded vectors)\n",
        "# ... (adjust this section based on your specific requirements)\n",
        "\n",
        "# Further processing (optional)\n",
        "# ... (e.g., data transformations, model training, etc.)\n",
        "print(label_to_index)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tzlpo6nuPlX4",
        "outputId": "d4dcc678-aed8-44c6-cf22-72fc2184b4b8"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'pants green': 0, 'shoes black': 1, 'shirt blue': 2, 'shoes green': 3, 'dress white': 4, 'shirt black': 5, 'dress black': 6, 'dress blue': 7, 'shorts brown': 8, 'shorts black': 9, 'pants red': 10, 'shoes brown': 11, 'pants brown': 12, 'shoes white': 13, 'shoes blue': 14, 'pants black': 15, 'shoes red': 16, 'shirt green': 17, 'pants blue': 18, 'shorts blue': 19, 'pants white': 20, 'shorts white': 21, 'dress red': 22, 'shorts green': 23}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# hot encoding\n",
        "\n",
        "num_classes_C1 = len(set(C1))  # Get number of unique classes for C1\n",
        "num_classes_C2 = len(set(C2))  # Get number of unique classes for C2\n",
        "\n",
        "# One-hot encode labels\n",
        "encoded_labels = []\n",
        "for label in zip(C1, C2):\n",
        "    C1_label, C2_label = label\n",
        "    C1_one_hot = F.one_hot(torch.tensor([C1_label_to_index(C1_label)]), num_classes=num_classes_C1)\n",
        "    C2_one_hot = F.one_hot(torch.tensor([C2_label_to_index(C2_label)]), num_classes=num_classes_C2)\n",
        "    encoded_labels.append((C1_one_hot, C2_one_hot))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "id": "Rcz6k6cnCSuI",
        "outputId": "d4f866f7-b64f-4bf5-e1dc-ca6ae7fc60ac"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "too many dimensions 'str'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-dc9d38876c8e>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Convert C1 and C2 labels to tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mC1_train_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mC1_label\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mC1_label\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mC1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mC2_train_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mC2_label\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mC2_label\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mC2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: too many dimensions 'str'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define total number of images\n",
        "total_images = len(images)\n",
        "\n",
        "# Define split proportions (ratios should sum to 1)\n",
        "train_ratio = 0.75\n",
        "val_ratio = 0.15\n",
        "test_ratio = 1 - train_ratio - val_ratio  # Ensure all images are used\n",
        "\n",
        "# Calculate split lengths based on total images\n",
        "train_len = int(total_images * train_ratio)\n",
        "val_len = int(total_images * val_ratio)\n",
        "test_len = total_images - train_len - val_len\n",
        "\n",
        "print(train_len)\n",
        "print(val_len)\n",
        "print(test_len)\n",
        "print(train_len + val_len + test_len)\n",
        "print(len(images))\n",
        "print(len(C1))\n",
        "print(len(C2))\n",
        "print(len([C1, C2]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MF-_Sw466JQy",
        "outputId": "e683705d-dc21-467a-e2b7-2daed6073a75"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "382\n",
            "76\n",
            "52\n",
            "510\n",
            "510\n",
            "510\n",
            "510\n",
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels = list(zip(C1, C2))  # Combine C1 and C2 labels into a list of tuples\n",
        "\n",
        "# Split data using random_split\n",
        "train_data, val_data, test_data = random_split([images], [train_len, val_len, test_len])\n",
        "\n",
        "\n",
        "# Unpack the data\n",
        "X_train, (C1_train, C2_train) = train_data\n",
        "X_val, (C1_val, C2_val) = val_data\n",
        "X_test, (C1_test, C2_test) = test_data\n",
        "\n",
        "# Separate images and labels\n",
        "y_train = [C1_train, C2_train]\n",
        "y_val = [C1_val, C2_val]\n",
        "y_test = [C1_test, C2_test]\n",
        "\n",
        "# Convert C1 and C2 labels to tensors\n",
        "C1_train_tensor = torch.tensor([C1_label for C1_label in y_train[:, 0]])\n",
        "C2_train_tensor = torch.tensor([C2_label for C2_label in y_train[:, 1]])\n",
        "\n",
        "# One-hot encode C1 and C2\n",
        "num_classes_C1 = 5\n",
        "num_classes_C2 = 6\n",
        "C1_train_encoded = F.one_hot(C1_train_tensor, num_classes=num_classes_C1)\n",
        "C2_train_encoded = F.one_hot(C2_train_tensor, num_classes=num_classes_C2)\n",
        "\n",
        "# Combine encoded features and images\n",
        "X_train_encoded = torch.cat((X_train, C1_train_encoded, C2_train_encoded), dim=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "LwPZKxDbZQb2",
        "outputId": "97d6dc91-1a30-4d87-8c22-9d887b18527a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Sum of input lengths does not equal the length of the input dataset!",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-0516d641ec23>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Split data using random_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrain_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_len\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36mrandom_split\u001b[0;34m(dataset, lengths, generator)\u001b[0m\n\u001b[1;32m    417\u001b[0m     \u001b[0;31m# Cannot verify that dataset is Sized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Sum of input lengths does not equal the length of the input dataset!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandperm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type, call-overload]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Sum of input lengths does not equal the length of the input dataset!"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=1,\n",
        "                               out_channels=32,\n",
        "                               kernel_size = 3,\n",
        "                               stride=1,\n",
        "                               padding=0)\n",
        "        # dim (28-3)/1 +1 = 26\n",
        "        #  feature map dim 26x26x32\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
        "        # dim 24 x 24 x 64\n",
        "        self.maxpool2d = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        # dim 12x12 x 64, 'same' = 9216\n",
        "        self.dropout1 = nn.Dropout(0.25)\n",
        "        self.fc1 = nn.Linear(9216, 128)\n",
        "        self.dropout2 = nn.Dropout(0.25)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.maxpool2d(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc2(x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        return output\n",
        "\n",
        "#Creating the model\n",
        "model = CNN()\n",
        "print(model)\n",
        "\n",
        "# calling the method on one batch\n",
        "it = iter(train_dataloader)\n",
        "X_batch, y_batch = next(it)\n",
        "print(model.forward(X_batch).shape)\n",
        "\n",
        "# Computing the accuracy of a test set\n",
        "def evaluate(model, test_loader, error):\n",
        "#model = mlpinplace=False\n",
        "    correct = 0\n",
        "    cur_loss = 0;\n",
        "\n",
        "    for test_imgs, test_labels in test_loader:\n",
        "        test_imgs, test_labels = test_imgs.cuda(), test_labels.cuda()\n",
        "        output = model(test_imgs)\n",
        "        loss = error(output, test_labels)\n",
        "        cur_loss+=loss.item()\n",
        "        predicted = torch.max(output,1)[1]\n",
        "        correct += (predicted == test_labels).sum()\n",
        "    return cur_loss/len(test_loader.dataset), correct/len(test_loader.dataset)\n",
        "\n",
        "\n",
        "\n",
        "#training the model\n",
        "def fit(model, train_loader, error, test_loader, epochs):\n",
        "    optimizer = torch.optim.Adam(model.parameters(),lr=0.0001)#,lr=0.001, betas=(0.9,0.999))\n",
        "\n",
        "\n",
        "    model.train()\n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        cur_loss = 0;\n",
        "        correct = 0\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            # moving data to GPU\n",
        "            X_batch, y_batch = X_batch.cuda(), y_batch.cuda()\n",
        "            optimizer.zero_grad()\n",
        "            output = model(X_batch)\n",
        "            loss = error(output, y_batch)\n",
        "            cur_loss+= loss.item()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        train_losses.append(cur_loss/len(train_loader.dataset))\n",
        "\n",
        "        test_loss, _ = evaluate(model, test_loader, error)\n",
        "        test_losses.append(test_loss)\n",
        "        print(f'Epoch : {epoch},  train loss:{train_losses[-1]}, test loss:{test_losses[-1]}')\n",
        "\n",
        "    return train_losses, test_losses"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "id": "Z7qaXTRbZTJ6",
        "outputId": "28c66419-7b75-4dc8-acb9-00a74ccc3fc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'nn' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-b5d1f8d0d01d>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         self.conv1 = nn.Conv2d(in_channels=1,\n",
            "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train\n",
        "\n",
        "epochs = 15\n",
        "error = nn.CrossEntropyLoss() # Loss for multiclass classification\n",
        "train_losses, test_losses = fit(model,train_dataloader, error, test_dataloader, epochs)\n",
        "\n",
        "plt.plot(train_losses,label=\"training loss\")\n",
        "plt.plot(test_losses,label=\"testing loss\")\n",
        "plt.legend(fontsize=15)\n",
        "plt.xlabel('Epoch', fontsize=20)\n",
        "plt.ylabel('Loss', fontsize=20)\n",
        "plt.xticks(fontsize=15)\n",
        "plt.yticks(fontsize=15)\n",
        "plt.figure()"
      ],
      "metadata": {
        "id": "hrMWNW17ZUd_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# validate and test\n",
        "\n",
        "loss, acc = evaluate(model, train_dataloader, error)\n",
        "print(f\"Train Accuracy:{acc}, Train loss:{loss}\")\n",
        "\n",
        "loss, acc = evaluate(model, test_dataloader, error)\n",
        "print(f\"Test Accuracy:{acc}, Test loss:{loss}\")\n",
        "\n",
        "loss, acc = evaluate(model, val_dataloader, error)\n",
        "print(f\"Validation Accuracy:{acc}, Validation loss:{loss}\")"
      ],
      "metadata": {
        "id": "oIVAo80VZWpk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}