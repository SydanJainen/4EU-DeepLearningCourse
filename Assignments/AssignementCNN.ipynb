{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNe3ZDm9CJcExsBqQlrw+/G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SydanJainen/4EU-DeepLearningCourse/blob/main/Assignments/AssignementCNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMteOzeJ8hrL"
      },
      "source": [
        "# Assignments\n",
        "\n",
        "### We are given a dataset containing 510 images which are categorized according two main characteristics/properties: “C1” and “C2”. The following inputs are available:\n",
        "* #### `input image', a 110 × 110 × 3 real-valued tensor. The last dimension denotes the number of input channels; the images belong to different combinations of the two properties C1 and C2. For C1 and C2 we have 5 and 6 possible values, respectively;  among the 30 possible couples (C1,C2), we have images for only 24 of them. We have around 20 samples (little more or less) for each of the available couples;  values in each tensor entry are integers in [0, 255];\n",
        "* #### C1, string, the value for the C1 property. 5 possible values;\n",
        "* #### C2, string, the value for the C2 property. 6 possible values.\n",
        "\n",
        "### Design a deep neural network model to predict the class of an image, given by the couple (C1,C2).\n",
        "\n",
        "### Provide a sketch of each of the following points, then implement your solution.\n",
        "1. MODEL: Which architecture do you consider the most appropriate for this task, and why;\n",
        "2. INPUT:\n",
        "* After a potential preprocessing step, which is the input of the model, and\n",
        "how is it represented;\n",
        "3. OUTPUT: How would you design the output layer and why;\n",
        "4. LOSS: Which loss function would you use to train your model and why;\n",
        "5. MODEL CONFIGURATION:\n",
        " Model composition (composition of layers, regardless their number,\n",
        "or their dimension, which can be object of tuning)\n",
        "6. MODEL EVALUATION: How would you assess (in which setting) the\n",
        "generalization capabilities of the model on unseen data?\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "## go to your datapath\n",
        "%cd /content/drive/MyDrive/INSEGNAMENTI/4EU+/\n",
        "\n",
        "# Here you should see the desired files\n",
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OI9U1PiwZ8EJ",
        "outputId": "d0ae871f-2e3f-4765-b971-7a400ad43761"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/INSEGNAMENTI/4EU+\n",
            "input.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ApgM0K_w8hrQ",
        "outputId": "ee0a7b84-9c6b-448f-94a9-980e5f74e66b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length image:510\n",
            "length C1:510\n",
            "length C2:510\n"
          ]
        }
      ],
      "source": [
        "import pickle as pk\n",
        "# Open the pickle data\n",
        "with open('input.pkl', 'rb') as f:\n",
        "  data = pk.load(f)\n",
        "\n",
        "images = data['imgs']\n",
        "C1 = data['C1']\n",
        "C2 = data['C2']\n",
        "print(f\"length image:{len(images)}\")\n",
        "print(f\"length C1:{len(C1)}\")\n",
        "print(f\"length C2:{len(C2)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import torchvision  #to get the MNIST dataset\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.transforms import ToTensor, Normalize, Compose\n",
        "from torch.utils.data import random_split"
      ],
      "metadata": {
        "id": "E5t2r4xPY7xy"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pre process\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "# Define split proportions\n",
        "train_ratio = 0.75\n",
        "val_ratio = 0.15\n",
        "test_ratio = 0.1\n",
        "\n",
        "# for reproducibility\n",
        "torch.manual_seed(1)\n",
        "\n",
        "# Split the entire dataset at once\n",
        "train_len = int(len(images) * train_ratio)\n",
        "val_len = int(len(images) * val_ratio)\n",
        "test_len = len(images) - train_len - val_len\n",
        "\n",
        "print(train_len)\n",
        "print(val_len)\n",
        "print(test_len)\n",
        "print(train_len + val_len + test_len)\n",
        "len(images)\n",
        "len(C1)\n",
        "len(C2)\n",
        "len([C1, C2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MF-_Sw466JQy",
        "outputId": "290c1123-162b-47f0-cb48-ba198fa7f1ca"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "382\n",
            "76\n",
            "52\n",
            "510\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data using random_split\n",
        "train_data, val_data, test_data = random_split([images, [C1, C2]], [train_len, val_len, test_len])\n",
        "\n",
        "# Unpack the data\n",
        "X_train, (C1_train, C2_train) = train_data\n",
        "X_val, (C1_val, C2_val) = val_data\n",
        "X_test, (C1_test, C2_test) = test_data\n",
        "\n",
        "# Separate images and labels (assuming C1 and C2 are combined)\n",
        "y_train = [C1_train, C2_train]\n",
        "y_val = [C1_val, C2_val]\n",
        "y_test = [C1_test, C2_test]\n",
        "\n",
        "# Convert C1 and C2 labels to tensors\n",
        "C1_train_tensor = torch.tensor([C1_label for C1_label in y_train[:, 0]])\n",
        "C2_train_tensor = torch.tensor([C2_label for C2_label in y_train[:, 1]])\n",
        "\n",
        "# One-hot encode C1 and C2\n",
        "num_classes_C1 = 5\n",
        "num_classes_C2 = 6\n",
        "C1_train_encoded = F.one_hot(C1_train_tensor, num_classes=num_classes_C1)\n",
        "C2_train_encoded = F.one_hot(C2_train_tensor, num_classes=num_classes_C2)\n",
        "\n",
        "# Combine encoded features and images\n",
        "X_train_encoded = torch.cat((X_train, C1_train_encoded, C2_train_encoded), dim=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "LwPZKxDbZQb2",
        "outputId": "c1d8ac06-a2d5-4418-a660-a98386fa3bd7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Sum of input lengths does not equal the length of the input dataset!",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-a9d4422d977b>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Split data using random_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mC1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrain_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_len\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Unpack the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mC1_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC2_train\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36mrandom_split\u001b[0;34m(dataset, lengths, generator)\u001b[0m\n\u001b[1;32m    417\u001b[0m     \u001b[0;31m# Cannot verify that dataset is Sized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Sum of input lengths does not equal the length of the input dataset!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandperm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type, call-overload]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Sum of input lengths does not equal the length of the input dataset!"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=1,\n",
        "                               out_channels=32,\n",
        "                               kernel_size = 3,\n",
        "                               stride=1,\n",
        "                               padding=0)\n",
        "        # dim (28-3)/1 +1 = 26\n",
        "        #  feature map dim 26x26x32\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
        "        # dim 24 x 24 x 64\n",
        "        self.maxpool2d = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        # dim 12x12 x 64, 'same' = 9216\n",
        "        self.dropout1 = nn.Dropout(0.25)\n",
        "        self.fc1 = nn.Linear(9216, 128)\n",
        "        self.dropout2 = nn.Dropout(0.25)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.maxpool2d(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc2(x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        return output\n",
        "\n",
        "#Creating the model\n",
        "model = CNN()\n",
        "print(model)\n",
        "\n",
        "# calling the method on one batch\n",
        "it = iter(train_dataloader)\n",
        "X_batch, y_batch = next(it)\n",
        "print(model.forward(X_batch).shape)\n",
        "\n",
        "# Computing the accuracy of a test set\n",
        "def evaluate(model, test_loader, error):\n",
        "#model = mlpinplace=False\n",
        "    correct = 0\n",
        "    cur_loss = 0;\n",
        "\n",
        "    for test_imgs, test_labels in test_loader:\n",
        "        test_imgs, test_labels = test_imgs.cuda(), test_labels.cuda()\n",
        "        output = model(test_imgs)\n",
        "        loss = error(output, test_labels)\n",
        "        cur_loss+=loss.item()\n",
        "        predicted = torch.max(output,1)[1]\n",
        "        correct += (predicted == test_labels).sum()\n",
        "    return cur_loss/len(test_loader.dataset), correct/len(test_loader.dataset)\n",
        "\n",
        "\n",
        "\n",
        "#training the model\n",
        "def fit(model, train_loader, error, test_loader, epochs):\n",
        "    optimizer = torch.optim.Adam(model.parameters(),lr=0.0001)#,lr=0.001, betas=(0.9,0.999))\n",
        "\n",
        "\n",
        "    model.train()\n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        cur_loss = 0;\n",
        "        correct = 0\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            # moving data to GPU\n",
        "            X_batch, y_batch = X_batch.cuda(), y_batch.cuda()\n",
        "            optimizer.zero_grad()\n",
        "            output = model(X_batch)\n",
        "            loss = error(output, y_batch)\n",
        "            cur_loss+= loss.item()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        train_losses.append(cur_loss/len(train_loader.dataset))\n",
        "\n",
        "        test_loss, _ = evaluate(model, test_loader, error)\n",
        "        test_losses.append(test_loss)\n",
        "        print(f'Epoch : {epoch},  train loss:{train_losses[-1]}, test loss:{test_losses[-1]}')\n",
        "\n",
        "    return train_losses, test_losses"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "id": "Z7qaXTRbZTJ6",
        "outputId": "28c66419-7b75-4dc8-acb9-00a74ccc3fc9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'nn' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-b5d1f8d0d01d>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         self.conv1 = nn.Conv2d(in_channels=1,\n",
            "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train\n",
        "\n",
        "epochs = 15\n",
        "error = nn.CrossEntropyLoss() # Loss for multiclass classification\n",
        "train_losses, test_losses = fit(model,train_dataloader, error, test_dataloader, epochs)\n",
        "\n",
        "plt.plot(train_losses,label=\"training loss\")\n",
        "plt.plot(test_losses,label=\"testing loss\")\n",
        "plt.legend(fontsize=15)\n",
        "plt.xlabel('Epoch', fontsize=20)\n",
        "plt.ylabel('Loss', fontsize=20)\n",
        "plt.xticks(fontsize=15)\n",
        "plt.yticks(fontsize=15)\n",
        "plt.figure()"
      ],
      "metadata": {
        "id": "hrMWNW17ZUd_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# validate and test\n",
        "\n",
        "loss, acc = evaluate(model, train_dataloader, error)\n",
        "print(f\"Train Accuracy:{acc}, Train loss:{loss}\")\n",
        "\n",
        "loss, acc = evaluate(model, test_dataloader, error)\n",
        "print(f\"Test Accuracy:{acc}, Test loss:{loss}\")\n",
        "\n",
        "loss, acc = evaluate(model, val_dataloader, error)\n",
        "print(f\"Validation Accuracy:{acc}, Validation loss:{loss}\")"
      ],
      "metadata": {
        "id": "oIVAo80VZWpk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}